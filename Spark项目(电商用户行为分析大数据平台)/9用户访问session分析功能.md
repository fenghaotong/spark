# 用户访问session分析

### session聚合统计

统计出来之前通过条件过滤的session，访问时长在0s~3s的session的数量，占总session数量的比例；4s~6s的session的数量，占总session数量的比例等等；访问步长在1~3的session的数量，占总session数量的比例；4~6的session的数量，占总session数量的比例等等；

```java
Accumulator 1s_3s = sc.accumulator(0L);
Accumulator 4s_6s = sc.accumulator(0L);
...
// 十几个Accumulator
```

- 可以对过滤以后的session，调用foreach也可以，遍历所有session；计算每个session的访问时长和访问步长；
- 访问时长：把session的最后一个action的时间，减去第一个action的时间
- 访问步长：session的action数量
- 计算出访问时长和访问步长以后，根据对应的区间，找到对应的Accumulator，1s_3s.add(1L)
- 同时每遍历一个session，就可以给总session数量对应的Accumulator，加1
- 最后用各个区间的session数量，除以总session数量，就可以计算出各个区间的占比了

**这种传统的实现方式，有什么不好**

- 最大的不好，就是Accumulator太多了，不便于维护
- 首先第一，很有可能，在写后面的累加代码的时候，比如找到了一个4s~6s的区间的session，但是却代码里面不小心，累加到7s~9s里面去了；
- 第二，当后期，项目如果要出现一些逻辑上的变更，比如说，session数量的计算逻辑，要改变，就得更改所有Accumulator对应的代码；或者说，又要增加几个范围，那么又要增加多个Accumulator，并且修改对应的累加代码；维护成本，相当之高（甚至可能，修改一个小功能，或者增加一个小功能，耗费的时间，比做一个新项目还要多；甚至于，还修改出了bug，那就耗费更多的时间）

所以，我们这里的设计，不打算采用传统的方式，用十几个，甚至二十个Accumulator，因为维护成本太高，这里的实现思路是，我们自己自定义一个Accumulator，实现较为复杂的计算逻辑，一个Accumulator维护了所有范围区间的数量的统计逻辑低耦合，如果说，session数量计算逻辑要改变，那么不用变更session遍历的相关的代码；只要维护一个Accumulator里面的代码即可；如果计算逻辑后期变更，或者加了几个范围，那么也很方便，不用多加好几个Accumulator，去修改大量的代码；只要维护一个Accumulator里面的代码即可；维护成本，大大降低

**自定义Accumulator**

- 自定义Accumulator，也是Spark Core中，属于比较高端的一个技术
- 使用自定义Accumulator，就可以任意的实现自己的复杂分布式计算的逻辑
- 如果说，你的task，分布式，进行复杂计算逻辑，那么是很难实现的（借助于redis，维护中间状态，借助于zookeeper去实现分布式锁）但是，使用自定义Accumulator，可以更方便进行中间状态的维护，而且不用担心并发和锁的问题

**重构实现思路与重构session聚合**

- 如果不进行重构，直接来实现，思路：
  1. actionRDD，映射成<sessionid,Row>的格式
  2. 按sessionid聚合，计算出每个session的访问时长和访问步长，生成一个新的RDD
  3. 遍历新生成的RDD，将每个session的访问时长和访问步长，去更新自定义Accumulator中的对应的值

  4. 使用自定义Accumulator中的统计值，去计算各个区间的比例
  5. 将最后计算出来的结果，写入MySQL对应的表中

- 普通实现思路的问题：
  1. 为什么还要用actionRDD，去映射？其实我们之前在session聚合的时候，映射已经做过了。多此一举
  2. 是不是一定要，为了session的聚合这个功能，单独去遍历一遍session？其实没有必要，已经有session数据之前过滤session的时候，其实，就相当于，是在遍历session，那么这里就没有必要再过滤一遍了

- 重构实现思路：
  1. 不要去生成任何新的RDD（处理上亿的数据）
  2. 不要去单独遍历一遍session的数据（处理上千万的数据）
  3. 可以在进行session聚合的时候，就直接计算出来每个session的访问时长和访问步长
  4. 在进行过滤的时候，本来就要遍历所有的聚合session信息，此时，就可以在某个session通过筛选条件后将其访问时长和访问步长，累加到自定义的Accumulator上面去
  5. 就是两种截然不同的思考方式，和实现方式，在面对上亿，上千万数据的时候，甚至可以节省时间长达半个小时，或者数个小时

- 开发Spark大型复杂项目的一些经验准则：
  1. 尽量少生成RDD
  2. 尽量少对RDD进行算子操作，如果有可能，尽量在一个算子里面，实现多个需要做的功能
  3. 尽量少对RDD进行shuffle算子操作，比如groupByKey、reduceByKey、sortByKey（map、mapToPair）shuffle操作，会导致大量的磁盘读写，严重降低性能，有shuffle的算子，和没有shuffle的算子，甚至性能，会达到几十分钟，甚至数个小时的差别，有shfufle的算子，很容易导致数据倾斜，一旦数据倾斜，简直就是性能杀手（完整的解决方案）
  4. 无论做什么功能，性能第一，在传统的J2EE或者.NET后者PHP，软件/系统/网站开发中，我认为是架构和可维护性，可扩展性的重要程度，远远高于了性能，大量的分布式的架构，设计模式，代码的划分，类的划分（高并发网站除外）
  5. 在大数据项目中，比如MapReduce、Hive、Spark、Storm，我认为性能的重要程度，远远大于一些代码的规范，和设计模式，代码的划分，类的划分；大数据，大数据，最重要的，就是性能，主要就是因为大数据以及大数据项目的特点，决定了，大数据的程序和项目的速度，都比较慢，如果不优先考虑性能的话，会导致一个大数据处理程序运行时间长度数个小时，甚至数十个小时，此时，对于用户体验，简直就是一场灾难
  6. 所以，推荐大数据项目，在开发和代码的架构中，优先考虑性能；其次考虑功能代码的划分、解耦合

我们如果采用第一种实现方案，那么其实就是代码划分（解耦合、可维护）优先，设计优先，如果采用第二种方案，那么其实就是性能优先

项目，最重要的，除了技术本身和项目经验以外；非常重要的一点，就是积累了，处理各种问题的经验

**重构过滤进行统计**

**计算统计结果并写入MySQL**

**本地测试**

**使用Scala实现自定义Accumulator**

### 随机抽取

**实现思路分析**

每一次执行用户访问session分析模块，要抽取出100个session

- session随机抽取：按每天的每个小时的session数量，占当天session总数的比例，乘以每天要抽取的session数量，计算出每个小时要抽取的session数量；然后呢，在每天每小时的session中，随机抽取出之前计算出来的数量的session。
- 举例：10000个session，100个session；0点~1点之间，有2000个session，占总session的比例就是0.2；按照比例，0点~1点需要抽取出来的session数量是100 * 0.2 = 20个；在0点~点的2000个session中，随机抽取出来20个session。

之前有的数据：session粒度的聚合数据（计算出来session的start_time）

- session聚合数据进行映射，将每个session发生的yyyy-MM-dd_HH（start_time）作为key，value就是session_id
- 对上述数据，使用countByKey算子，就可以获取到每天每小时的session数量
- （按时间比例随机抽取算法）每天每小时有多少session，根据这个数量计算出每天每小时的session占比，以及按照占比，需要抽取多少session，可以计算出每个小时内，从0~session数量之间的范围中，获取指定抽取数量个随机数，作为随机抽取的索引
- 把之前转换后的session数据（以yyyy-MM-dd_HH作为key），执行groupByKey算子；然后可以遍历每天每小时的session，遍历时，遇到之前计算出来的要抽取的索引，即将session抽取出来；抽取出来的session，直接写入MySQL数据库

**计算每天每小时session数量**

**按时间比例随机抽取算法实现**

**根据随机索引进行抽取**

**获取抽取session的明细数据**

**本地测试**



### top10热门品类

**实现需求**

- 需求回顾：top10热门品类
- 计算出来通过筛选条件的那些session，他们访问过的所有品类（点击、下单、支付），按照各个品类的点击、下单和支付次数，降序排序，获取前10个品类，也就是筛选条件下的那一批session的top10热门品类；点击、下单和支付次数：优先按照点击次数排序、如果点击次数相等，那么按照下单次数排序、如果下单次数相当，那么按照支付次数排序
- 这个需求是很有意义的，因为这样，就可以让数据分析师、产品经理、公司高层，随时随地都可以看到自己感兴趣的那一批用户，最喜欢的10个品类，从而对自己公司和产品的定位有清晰的了解，并且可以更加深入的了解自己的用户，更好的调整公司战略

二次排序：

- 如果我们就只是根据某一个字段进行排序，比如点击次数降序排序，那么就不是二次排序；二次排序，顾名思义，就是说，不只是根据一个字段进行一次排序，可能是要根据多个字段，进行多次排序的，点击、下单和支付次数，依次进行排序，就是二次排序
- sortByKey算子，默认情况下，它支持根据int、long等类型来进行排序，但是那样的话，key就只能放一个字段了，所以需要自定义key，作为sortByKey算子的key，自定义key中，封装n个字段，并在key中，自己在指定接口方法中，实现自己的根据多字段的排序算法，然后再使用sortByKey算子进行排序，那么就可以按照我们自己的key，使用多个字段进行排序

本模块中，最最重要和核心的一个Spark技术点

- 实现思路分析：
  1. 拿到通过筛选条件的那批session，访问过的所有品类id
  2. 计算出session访问过的所有品类的点击、下单和支付次数，这里可能要跟第一步计算出来的品类进行join
  3. 自己开发二次排序的key
  4. 做映射，将品类的点击、下单和支付次数，封装到二次排序key中，作为PairRDD的key
  5. 使用sortByKey(false)，按照自定义key，进行降序二次排序
  6. 使用take(10)获取，排序后的前10个品类，就是top10热门品类
  7. 将top10热门品类，以及每个品类的点击、下单和支付次数，写入MySQL数据库
  8. 本地测试
  9. 使用Scala来开发二次排序key

**获取session访问过的所有品类**

**计算各品类点击、下单和支付的次数**

**join品类与点击下单支付次数**

**自定义二次排序key**

**进行二次排序**

**获取top10品类并写入MySQL**

**本地测试**

**使用Scala实现二次排序**



### top10活跃session

**需求与思路**

需求回顾：top10活跃session

- top10热门品类，获取每个品类点击次数最多的10个session，以及其对应的访问明细

实现思路分析：

1. 拿到符合筛选条件的session的明细数据
2. 按照session粒度进行聚合，获取到session对每个品类的点击次数，用flatMap，算子函数返回的是`<categoryid,(sessionid,clickCount)>`
3. 按照品类id，分组取top10，获取到top10活跃session；groupByKey；自己写算法，获取到点击次数最多的前10个session，直接写入MySQL表；返回的是sessionid
4. 获取各品类top10活跃session的访问明细数据，写入MySQL
5. 本地测试

新增：

1. 重构一下之前的代码，将通过筛选条件的session的访问明细数据RDD，提取成公共的RDD；这样就不用重复计算同样的RDD
2. 将之前计算出来的top10热门品类的id，生成一个PairRDD，方便后面进行join

**计算top10品类被各sessoin点击的次数**

**分组取TopN算法获取top10活跃session**

**本地测试**

### 总结

**公共组件**

- 配置管理组件
- JDBC辅助组件
- 工具类
- 模拟数据生成程序
- 单元测试
- domain、dao



**第一个模块：用户访问session分析模块**

- 基础：session粒度聚合、按筛选条件进行过滤
- session聚合统计：统计出访问时长和访问步长，各个区间范围的session数量，占总session数量的比例
- session随机抽取：按时间比例，随机抽取出100个session
- top10热门品类：获取通过筛选条件的session，点击、下单和支付次数最多的10个品类
- top10活跃session：获取top10热门品类中，每个品类点击次数最多的10个session



**技术点和知识点**

-  正规的大型大数据项目的架构（公共组件的封装、包的划分、代码的规范）
- 复杂的大数据分析需求（纯spark作业代码，1500行+）
- Spark Core大部分算子在实际项目中的综合应用实战：map、reduce、count、group
- 高级技术点：自定义Accumulator、按时间比例随机抽取算法、二次排序、分组取TopN算法
- 标准和正规的大数据项目开发流程：数据调研、需求分析、技术方案设计、数据库设计、编码实现、单元测试、本地测试

**剩下的流程：性能调优、生产环境测试**

**接下来要做什么？：**

- 性能调优：按照本人开发过的大量的单个spark作业，处理10亿到100亿级别数据的经验，要针对我们写好的spark作业程序，实施十几个到二十个左右的复杂性调优技术；性能调优相关的原理讲解；性能调优技术的实施；实际经验中应用性能调优技术的经验总结；掌握一整套复杂的Spark企业级性能调优解决方案；而不只是简单的一些性能调优技巧（网上一些博客、其他一些视频、其他一些书）
-  数据倾斜解决方案：针对写好的spark作业，实施一整套数据倾斜解决方案：实际经验中积累的数据倾斜现象的表现，以及处理后的效果总结
- troubleshooting：针对写好的spark作业，讲解实际经验中遇到的各种线上报错问题，以及解决方案
- 生产环境测试：Hive表