# 离线和实时日志采集流程介绍

## 离线日志采集流程介绍

**数据来源**

- 互联网行业：网站、app、系统（交易系统。。）
- 传统行业：电信，人们的上网、打电话、发短信等等数据
- 数据源：网站、app
- 都要往我们的后台去发送请求，获取数据，执行业务逻辑；app获取要展现的商品数据；发送请求到后台进行交易和结账

>  网站/app会发送请求到后台服务器，通常会由Nginx接收请求，并进行转发

**后台服务器**

- 后台服务器，比如Tomcat、Jetty；但是，其实在面向大量用户，高并发（每秒访问量过万）的情况下，通常都不会直接是用Tomcat来接收请求。这种时候，通常，都是用Nginx来接收请求，并且后端接入Tomcat集群/Jetty集群，来进行高并发访问下的负载均衡。
- 比如说，Nginx，或者是Tomcat，你进行适当配置之后，所有请求的数据都会作为log存储起来；接收请求的后台系统（J2EE、PHP、Ruby On Rails），也可以按照你的规范，每接收一个请求，或者每执行一个业务逻辑，就往日志文件里面打一条log。

> 到这里为止，我们的后台每天就至少可以产生一份日志文件，这个是没有疑问了

**日志文件**

- 日志文件（通常由我们预先设定的特殊的格式）通常每天一份。此时呢，由于可能有多份日志文件，因为有多个web服务器。

> 一个日志转移的工具，比如自己用linux的crontab定时调度一个shell脚本/python脚本；或者自己用java开发一个后台服务，用quartz这样的框架进行定时调度。这个工具，负责将当天的所有日志的数据，都给采集起来，进行合并和处理，等操作；然后作为一份日志文件，给转移到flume agent正在监控的目录中。

**flume**

- 按照我们上节课所讲的；flume agent启动起来以后，可以实时的监控linux系统上面的某一个目录，看其中是否有新的文件进来。只要发现有新的日志文件进来，那么flume就会走后续的channel和sink。通常来说，sink都会配置为HDFS。

> flume负责将每天的一份log文件，传输到HDFS上

**HDFS**

- Hadoop Distributed File System。Hadoop分布式文件系统。用来存储每天的log数据。为什么用hadoop进行存储呢。因为Hadoop可以存储大数据，大量数据。比如说，每天的日志，数据文件是一个T，那么，也许一天的日志文件，是可以存储在某个Linux系统上面，但是问题是，1个月的呢，1年的呢。当积累了大量数据以后，就不可能存储在单机上，只能存储在Hadoop大数据分布式存储系统中。

> 使用Hadoop MapReduce，自己开发MR作业，可以用crontab定时调度工具来定时每天执行一次；也可以用Oozie来进行定时调度；也可以（百度、阿里、腾讯、京东、美团）自己组建团队来研发复杂、大型、分布式的调度系统，来承担全公司所有MapReduce / Hive作业的调度（对于大型公司来说，可能每天除了负责数据清洗的MR作业以外，后续的建立数据仓库、进行数据分析和统计的Hive ETL作业可能高达上万个，上十万、百万个），针对HDFS里的原始日志进行数据清洗，写入HDFS中另外一个文件

- Hadoop HDFS中的原始的日志数据，会经过数据清洗。为什么要进行数据清洗？因为我们的数据中可能有很多是不符合预期的脏数据。HDFS：存储一份经过数据清洗的日志文件。

> 把HDFS中的清洗后的数据，给导入到Hive的某个表中。这里可以使用动态分区，Hive使用分区表，每个分区放一天的数据。

**Hive**

- Hive，底层也是基于HDFS，作为一个大数据的数据仓库。数据仓库内部，再往后，其实就是一些数据仓库建模的ETL。ETL会将原始日志所在的一个表，给转换成几十张，甚至上百张表。这几十，甚至上百张表，就是我们的数据仓库。然后呢，公司的统计分析人员，就会针对数据仓库中的表，执行临时的，或者每天定时调度的Hive SQL ETL作业。来进行大数据的统计和分析。

> Spark/Hdoop/Storm，大数据平台/系统，可能都会使用Hive中的数据仓库内部的表

**Spark平台**

- Spark大型大数据平台/系统，其实，通常来说，都会针对Hive中的数据来进行开发。也就是说，我们的Spark大数据系统，数据来源都是Hive中的某些表。这些表，可能都是经过大量的Hive ETL以后建立起来的数据仓库中的某些表。然后来开发特殊的，符合业务需求的大数据平台。通过大数据平台来给公司里的用户进行使用，来提供大数据的支持，推动公司的发展。

## 实时日志采集流程介绍

**数据来源**

- 比如，网站或者app。非常重要的一点，就是埋点。也就是说，埋点，在网站/app的哪个页面的哪些操作发生时，前端的代码（网站，JavaScript；app，android/IOS），就通过网络请求，（Ajax；socket），向后端的服务器发送指定格式的日志数据。

**后台服务器**

- Nginx，后台Web服务器（Tomcat、Jetty），后台系统（J2EE、PHP）。到这一步为止，其实还是可以跟我们之前的离线日志收集流程一样。走后面的通过一个日志传输工具，给放入指定的文件夹。

  > 可以通过flume，转移到HDFS中

**kafka**

- 实时数据，通常都是从分布式消息队列集群中读取的，比如Kafka；实时数据，实时的log，实时的写入到消息队列中，比如Kafka；然后呢，再由我们后端的实时数据处理程序（Storm、Spark Streaming），实时从Kafka中读取数据，log日志。然后进行实时的计算和处理。

> Kafka，我们的日志数据，怎么处理，都是由自己决定。可以每天收集一份，放到flume，转移到HDFS里面，清洗后放入Hive，建立离线的数据仓库。
>
> 也可以每收集1分钟的数据，或者每收集一点数据，就放入文件，然后转移到flume中去，或者直接通过API定制，直接把一条一条的log打入flume。可以配置flume，将数据写入Kafka
>
> 实时的，主动从Kafka中拉取数据

**大数据实时计算系统**

- 大数据实时计算系统，比如说用Storm、Spark Streaming开发的，可以实时的从Kafka中拉取数据，然后对实时的数据进行处理和计算，这里可以封装大量复杂的业务逻辑，甚至调用复杂的机器学习、数据挖掘、智能推荐的算法，然后实现实时的车辆调度、实时推荐。