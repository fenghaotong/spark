# 需求分析、技术方案设计以及数据设计

**需求：根据用户指定的日期范围，统计各个区域下的最热门的top3商品**

- Spark作业接收taskid，查询对应的MySQL中的task，获取用户指定的筛选参数；统计出指定日期范围内的，各个区域的top3热门商品；最后将结果写入MySQL表中。

**技术方案设计：**

1. 查询task，获取日期范围，通过`Spark SQL`，查询`user_visit_action`表中的指定日期范围内的数据，过滤出，商品点击行为，`click_product_id is not null；click_product_id != 'NULL'；click_product_id != 'null'；city_id，click_product_id`
2. 使用Spark SQL从MySQL中查询出来城市信息`（city_id、city_name、area）`，用户访问行为数据要跟城市信息进行`join，city_id、city_name、area、product_id，RDD`，转换成DataFrame，注册成一个临时表
3. Spark SQL内置函数`（case when`），对area打标记（华东大区，A级，华中大区，B级，东北大区，C级，西北大区，D级），`area_level`
4. 计算出来每个区域下每个商品的点击次数，`group by area, product_id`；保留每个区域的城市名称列表；自定义`UDAF，group_concat_distinct()`函数，聚合出来一个`city_names`字段，`area、product_id、city_names、click_count`
5. join商品明细表，`hive（product_id、product_name、extend_info）`，`extend_info`是json类型，自定义UDF，`get_json_object()`函数，取出其中的`product_status`字段，`if()`函数（Spark SQL内置函数），判断，0 自营，1 第三方；`（area、product_id、city_names、click_count、product_name、product_status）`
6. 开窗函数，根据area来聚合，获取每个area下，`click_count`排名前3的product信息；`area、area_level、product_id、city_names、click_count、product_name、product_status`
7. 结果写入MySQL表中
8. Spark SQL的数据倾斜解决方案？双重`group by`、随机key以及扩容表（自定义UDF函数，`random_key()）、Spark SQL`内置的`reduce join`转换为`map join`、提高shuffle并行度
9. 本地测试和生产环境的测试

**基础数据的准备和设计**

1. MySQL表中，要有city_info，city_id、city_name、area
2. Hive表中，要有一个product_info表，product_id、product_name、extend_info
3. MySQL中，设计结果表，task_id、area、area_level、product_id、city_names、click_count、product_name、product_status

