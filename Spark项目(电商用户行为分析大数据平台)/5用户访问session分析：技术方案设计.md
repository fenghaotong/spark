# 用户访问session分析：技术方案设计

**需求**

1. 按条件筛选session
2. 统计出符合条件的session中，访问时长在1s~3s、4s~6s、7s~9s、10s~30s、30s~60s、1m~3m、3m~10m、10m~30m、30m以上各个范围内的session占比；访问步长在1~3、4~6、7~9、10~30、30~60、60以上各个范围内的session占比
3. 在符合条件的session中，按照时间比例随机抽取1000个session
4. 在符合条件的session中，获取点击、下单和支付数量排名前10的品类
5. 对于排名前10的品类，分别获取其点击次数排名前10的session

- 大数据项目开发流程的第三个步骤。就是说，在调研完了基础数据、分析完了需求之后，就需要针对我们手头上有的基础数据和PM提出来的需求，来进行技术方案的设计。所谓技术方案，指的就是，基于现有的数据，针对提出的需求，实现所有需求的整个技术架构、关键的技术点等。在这个过程中，需要考虑到实现所有需求，需要使用以及可能涉及到的技术点。另外，在这个过程中，有时也会涉及到技术的选项。比如，如果说，我们的Spark程序在中间，需要对某个RDD的数据写入外部的缓存，以便于后续的算子可以直接通过缓存读取数据。那么就需要对缓存进行技术选项，redis、memcached、spark tachyon。
- 技术架构：前端+J2EE+Spark+MySQL。
- 实现需求需要使用的以及涉及到的技术点，和技术实现思路，是我们这里的重点。也就是说，实现上述几个需求，你的技术实现的思路，以及在思路中，可能使用到的技术的要点。

**按条件筛选session**

- 第一个问题，要按条件筛选session，但是这个筛选的粒度是不同的，比如说搜索词、访问时间，那么这个都是session粒度的，甚至是action粒度的；还有针对用户的基础信息进行筛选，年龄、性别、职业。。；所以说筛选粒度是不统一的。
- 第二个问题，每天的用户访问数据量是很大的，因为user_visit_action这个表，一行就代表了用户的一个行为，比如点击或者搜索；在国内一个大的电商企业里面，如果每天的活跃用户数量在千万级别的话。那么，这个user_visit_action表，每天的数据量大概在至少5亿以上，在10亿左右
- 那么针对这个筛选粒度不统一的问题，以及数据量巨大（10亿/day），可能会有两个问题；首先第一个，就是，如果不统一筛选粒度的话，那么就必须得对所有的数据进行全量的扫描；第二个，就是全量扫描的话，量实在太大了，一天如果在10亿左右，那么10天呢（100亿），100呢，1000亿。量太大的话，会导致Spark作业的运行速度大幅度降低。极大的影响平台使用者的用户体验。
- 所以为了解决这个问题，那么我们选择在这里，对原始的数据，进行聚合，什么粒度的聚合呢？session粒度的聚合。也就是说，用一些最基本的筛选条件，比如时间范围，从hive表中提取数据，然后呢，按照session_id这个字段进行聚合，那么聚合后的一条记录，就是一个用户的某个session在指定时间内的访问的记录，比如搜索过的所有的关键词、点击过的所有的品类id、session对应的userid关联的用户的基础信息。
- 聚合过后，针对session粒度的数据，按照使用者指定的筛选条件，进行数据的筛选。筛选出来符合条件的用session粒度的数据。其实就是我们想要的那些session了。

**聚合统计**

- 如果要做这个事情，那么首先要明确，spark作业是分布式的。所以也就是说，每个spark task在执行我们的统计逻辑的时候，可能就需要对一个全局的变量，进行累加操作。比如代表访问时长在1s~3s的session数量，初始是0，然后呢分布式处理所有的session，判断每个session的访问时长，如果是1s~3s内的话，那么就给1s~3s内的session计数器，累加1。
- 那么在spark中，要实现分布式安全的累加操作，基本上只有一个最好的选择，就是Accumulator变量。但是，问题又来了，如果是基础的Accumulator变量，那么可能需要将近20个Accumulator变量，1s~3s、4s~6s。。。。；但是这样的话，就会导致代码中充斥了大量的Accumulator变量，导致维护变得更加复杂，在修改代码的时候，很可能会导致错误。比如说判断出一个session访问时长在4s~6s，但是代码中不小心写了一个bug（由于Accumulator太多了），比如说，更新了1s~3s的范围的Accumulator变量。导致统计出错。
- 所以，对于这个情况，那么我们就可以使用自定义Accumulator的技术，来实现复杂的分布式计算。也就是说，就用一个Accumulator，来计算所有的指标。

**在符合条件的session中，按照时间比例随机抽取1000个session**

- 技术上来说，就是要综合运用Spark的countByKey、groupByKey、mapToPair等算子，来开发一个复杂的按时间比例随机均匀采样抽取的算法。（大数据算法）

**在符合条件的session中，获取点击、下单和支付数量排名前10的品类**

- 这里的话呢，需要对每个品类的点击、下单和支付的数量都进行计算。然后呢，使用Spark的自定义Key二次排序算法的技术，来实现所有品类，按照三个字段，点击数量、下单数量、支付数量依次进行排序，首先比较点击数量，如果相同的话，那么比较下单数量，如果还是相同，那么比较支付数量。

**对于排名前10的品类，分别获取其点击次数排名前10的session**

- 这个需求，需要使用Spark的分组取TopN的算法来进行实现。也就是说对排名前10的品类对应的数据，按照品类id进行分组，然后求出每组点击数量排名前10的session。



**总结**

- 通过学习这个模块，通过业务功能的开发，还不说性能调优、troubleshooting、数据倾斜方面的东西。仅仅是业务功能的开发，可以掌握到的技术点：
  1. 通过底层数据聚合，来减少spark作业处理数据量，从而提升spark作业的性能（从根本上提升spark性能的技巧）
  2. 自定义Accumulator实现复杂分布式计算的技术
  3. Spark按时间比例随机抽取算法
  4. Spark自定义key二次排序技术
  5. Spark分组取TopN算法
  6. 通过Spark的各种功能和技术点，进行各种聚合、采样、排序、取TopN业务的实现