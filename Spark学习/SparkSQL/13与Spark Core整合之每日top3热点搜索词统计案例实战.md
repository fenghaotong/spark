# 与Spark Core整合之每日top3热点搜索词统计案例实战

## 案例需求

- 数据格式：

  日期 用户 搜索词 城市 平台 版本

- 需求：

1. 筛选出符合查询条件（城市、平台、版本）的数据
2. 统计出每天搜索uv排名前3的搜索词
3. 按照每天的top3搜索词的uv搜索总次数，倒序排序
4. 将数据保存到hive表中



## 实现思路分析

- 针对原始数据（HDFS文件），获取输入的RDD
- 使用filter算子，去针对输入RDD中的数据，进行数据过滤，过滤出符合查询条件的数据。
  - 普通的做法：直接在fitler算子函数中，使用外部的查询条件（Map），但是，这样做的话，是不是查询条件Map，会发送到每一个task上一份副本。（性能并不好）
  - 优化后的做法：将查询条件，封装为Broadcast广播变量，在filter算子中使用Broadcast广播变量进行数据筛选。
- 将数据转换为`“(日期_搜索词, 用户)”格式，然后呢，对它进行分组，然后再次进行映射，对每天每个搜索词的搜索用户进行去重操作，并统计去重后的数量，即为每天每个搜索词的uv。最后，获得“(日期_搜索词, uv)”`
- 将得到的每天每个搜索词的uv，RDD，映射为元素类型为Row的RDD，将该RDD转换为DataFrame
- 将DataFrame注册为临时表，使用Spark SQL的开窗函数，来统计每天的uv数量排名前3的搜索词，以及它的搜索uv，最后获取，是一个DataFrame
- 将DataFrame转换为RDD，继续操作，按照每天日期来进行分组，并进行映射，计算出每天的top3搜索词的搜索uv的总数，然后将uv总数作为key，将每天的top3搜索词以及搜索次数，拼接为一个字符串
- 按照每天的top3搜索总uv，进行排序，倒序排序
- 将排好序的数据，再次映射回来，变成`“日期_搜索词_uv”`的格式
- 再次映射为DataFrame，并将数据保存到Hive中即可

## 实战

- 用Java来实现，是因为整个案例过于复杂，如果再用Scala来实现的话，那么时间会耗费的很长，而且意义并不大
- 而且，我们通过Java开讲解，已经把数据格式、需求、具体实现思路、如何优化（broatcast），都讲解的非常清晰了，而且开发过程中，也做了大量的讲解；相信大家通过目前为止的讲解，已经知道应该如何开发这个复杂的案例和类似的需求了
- 所以，更好的一个做法，是将Scala版本实现，留给大家自己去做，作为课后作业
- 实际上，如果大家之前能够掌握我们讲的所有的内容，应该完全，是可以用Scala开发出这个程序的

这个案例，是完全从实际企业需求改造了一点点，抽取出来的，完全企业级实战。这种需求在实际工作中，可能并不是某个大数据分析系统的模块。但是更多的是，PM或老大的需求，要求每个月，跑一次，统计上个月，每天搜索uv前3的热词。那么其实，你可以用crontab来定时调度该shell脚本，每个月1号跑一次，跑上个月的数据。只要在filter中，过滤数据，即可。

[Java版本实例](src/java/DailyTop3Keyword.java)

