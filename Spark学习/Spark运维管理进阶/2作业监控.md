# 作业监控

对于Spark作业的监控，Spark给我们提供了很多种方式：Spark Web UI，Spark History Web UI，RESTFUL API以及Metrics。

### SparkWebUI以及监控实验



- 每提交一个Spark作业，并且启动SparkContext之后，都会启动一个对应的Spark Web UI服务。默认情况下Spark Web UI的访问地址是driver进程所在节点的4040端口。在Spark Web UI上会展示作业相关的详细信息，非常有用，是Spark作业监控的最主要的手段。

**Spark Web UI包括了以下信息：**

1. stage和task列表
2. RDD大小以及内存使用的概览
3. 环境信息
4. 作业对应的executor的信息

- 可以通过在浏览器中访问`http://<driver-node>:4040`地址，来进入Spark Web UI界面。如果多个driver在一个机器上运行，它们会自动绑定到不同的端口上。默认从4040端口开始，如果发现已经被绑定了，那么会选择4041、4042等端口，以此类推。

- 要注意的是，这些信息默认情况下仅仅在作业运行期间有效并且可以看到。一旦作业完毕，那么driver进程以及对应的web ui服务也会停止，我们就无法看到已经完成的作业的信息了。如果要在作业完成之后，也可以看到其Spark Web UI以及详细信息，那么就需要启用Spark的History Server。

**监控实验**

1. 通过spark-shell以standalone模式执行一个wordcount作业，通过直接访问4040端口以及从8080端口两种方式进入web ui。
2. 在作业运行完毕之后，再尝试看看作业的Web UI。
3. 通过spark-shell以yarn模式执行一个wordcount作业，并重复上述过程。

### standalone模式下查看历史作业的WebUI

默认情况下，一个作业运行完成之后，就再也无法看到其web ui以及执行信息了，在生产环境中，这对调试以及故障定位有影响。

如果要在作业执行完之后，还能看到其web ui，那么必须将作业的spark.eventLog.enabled属性设置为true，这个属性会告诉spark去记录该作业的所有要在web ui上展示的事件以及信息。

如果spark记录下了一个作业生命周期内的所有事件，那么就会在该作业执行完成之后，我们进入其web ui时，自动用记录的数据重新绘制作业的web ui。

**有3个属性我们可以设置**

1. spark.eventLog.enabled，必须设置为true
2. spark.eventLog.dir，默认是/tmp/spark-events，建议自己手动调整为其他目录，比如/usr/local/spark-event或是hdfs目录，必须手动创建
3. spark.eventLog.compress ，是否压缩数据，默认为false，建议可以开启压缩以减少磁盘空间占用

- 这些属性可以在提交一个作业的时候设置
- 如果想要对所有作业都启用该机制，那么可以在spark-defaults.conf文件中配置这三个属性

**实验**

1. 先看看之前的已经执行完成的作业，是否可以进入spark web ui界面
2. 关闭现有的master和worker进程
3. 修改spark-defaults.conf文件，配置上述三个属性，启用standalone模式下的作业历史信息记录，手动创建hdfs目录
4. 重新启动spark集群
5. 使用spark-shell提交一个作业，然后再次尝试进入spark web ui界面

> 注意：如果要让spark完成作业的事件记录，那么必须最后以sc.stop()结尾。

### 启动HistoryServer查看历史作业的Web UI



**修改spark-defaults.conf**

```sh
spark.eventLog.enabled  true
spark.eventLog.dir      hdfs://192.168.75.101:9000/event_log_dir
spark.eventLog.compress true
```

**spark-env.sh**

```sh
export SPARK_HISTORY_OPTS="-Dspark.history.ui.port=18080 -Dspark.history.retainedApplications=50 -Dspark.history.fs.logDirectory=hdfs://192.168.75.101:9000/event_log_dir"
```



务必预先创建好hdfs://192.168.75.101:9000/spark-events目录
而且要注意，spark.eventLog.dir与spark.history.fs.logDirectory指向的必须是同一个目录
因为spark.eventLog.dir会指定作业事件记录在哪里，spark.history.fs.logDirectory会指定从哪个目录中去读取作业数据

- 启动HistoryServer: `./sbin/start-history-server.sh`

- 访问地址: `192.168.75.101:18080`

实验

1. 停止集群
2. 配置spark-env.sh
3. 重启集群
4. 启动history server
5. 运行spark-shell，在standalone模式下和yarn模式下，分别执行一个作业
6. 通过`192.168.75.101:18080`的HistoryServer UI可以看到所有运行后的作业信息

### 使用curl+REST API进行作业监控

除了查看ui上的统计来监控作业，还可以通过Spark提供的REST API来获取作业信息，并进行作业监控。REST API就给我们自己开发Spark的一些监控系统或平台提供了可能。REST API是通过http协议发送的，并给我们返回JSON格式的数据。因此无论你是用java，还是python，亦或是php，都可以获取Spark的监控信息。

**运行中的作业以及history server中的历史作业，都可以获取到信息**

1. 如果是要获取运行中的作业的信息，可以通过`http://host:4040/api/v1/...`的方式来获取
2. 如果是要获取历史作业的信息，可以通过`http://host:18080/api/v1/...`的方式来获取

比如说，`http://192.168.75.101:18080/api/v1/applications`，就可以获取到所有历史作业的基本信息

**以下是所有API的说明**

|API|说明|
|:---|:----|
|/applications	|																获取作业列表|
|/applications/[app-id]/jobs	|													指定作业的job列表|
|/applications/[app-id]/jobs/[job-id]	|										指定job的信息|
|/applications/[app-id]/stages|													指定作业的stage列表|
|/applications/[app-id]/stages/[stage-id]|										指定stage的所有attempt列表|
|/applications/[app-id]/stages/[stage-id]/[stage-attempt-id]|						指定stage attempt的信息|
|/applications/[app-id]/stages/[stage-id]/[stage-attempt-id]/taskSummary			|指定stage attempt所有task的metrics统计信息|
|/applications/[app-id]/stages/[stage-id]/[stage-attempt-id]/taskList			|指定stage attempt的task列表|
|/applications/[app-id]/executors	|											指定作业的executor列表|
|/applications/[app-id]/storage/rdd		|										指定作业的持久化rdd列表|
|/applications/[app-id]/storage/rdd/[rdd-id]|										指定持久化rdd的信息|
|/applications/[app-id]/logs			|											下载指定作业的所有日志的压缩包|
|/applications/[app-id]/[attempt-id]/logs	|									下载指定作业的某次attempt的所有日志的压缩包|

- 当作业运行在yarn中时，每个作业都可能会尝试多次运行，所以上述的所有`[app-id]`都必须替换为`[app-id]/[attempt-id]`
- 这些API都非常便于让我们去基于它们开发各种监控系统或应用。特别是，spark保证以下几点: 

1. API永远不会因为版本的变更而更改
2. JSON中的字段用于不会被移除
3. 新的API接口可能会被增加
4. 已有API接口中可能会增加新的字段
5. API的新版本可能会作为新接口被添加进来。新版本的接口不要求向后兼容。
6. API版本可能会被删除掉，但是肯定是在一个相关的新API版本发布之后。

- 要注意的是，当查看运行中作业的UI时，`applications/[app-id]`还是需要提供的，尽管此时在那个4040端口上可能只有一个作业在运行。比如说，要查看正在运行的作业的job列表，可能需要使用以下`API: http://host:4040/api/v1/applications/[app-id]/jobs`
- 这主要是为了尽可能地复用API接口

**实验**

1. 安装curl工具，来发送http请求:` yum install -y curl`
2. 试一试以上的几个API，去获取standalone模式和yarn模式运行中的作业，以及历史作业的信息

### Spark Metrics系统以及自定义Metrics Sink

Spark有一套可配置的metrics系统，是基于Coda Hale Metrics类库实现的。该metrics系统允许用户将Spark的metrics统计指标上报到多种目标源（sink）中，包括http，jmx和csv文件。这个metrics系统是通过一个配置文件进行配置的，在`$SPARK_HOME`目录的`conf`目录下，用一个`metrics.properties`文件来配置。可以通过在`spark-defaults.conf`中配置`spark.metrics.conf`属性来配置自定义的文件路径。spark metrics依据不同的spark组件划分为了不同的实例。在每一个实例中，你都可以配置一系列的sink来指定该实例的metrics要上报到哪里去。

**以下实例是目前被支持的**

```
master: spark standalone master进程
applications: master中的组件，可以上报所有application的metrics
worker: spark standalone worker进程
executor: spark executor进程
driver: spark driver进程
```

每个实例都可以上报metrics到0个或多个sink中。sink被包含在了`org.apache.spark.metrics.sink`包下。

- ConsoleSink: 日志metrics，打印到控制台

- CSVSink: 以固定的频率将metrics数据导出到CSV文件中

- JmxSink: 注册metrics到JMX console中

- MetricsServlet: 在Spark UI中添加一个servlet来通过JSON数据提供metrics数据（之前的REST API就是通过该方式进行的）

- Slf4jSink: 以日志的形式发送metrics到slf4j

- GraphiteSink: 发送metrics到Graphite节点

- GangliaSink: 发送metrics到Ganglia节点。

  > Spark也支持Ganglia sink，但是没有包含在默认的打包内，因为有版权的问题。
  >
  > 要安装GangliaSink，就需要自己编译一个spark。要注意，必须要提供必要的授权信息。

**metrics系统的意义**

1. metrics只能在spark web ui上看到，或者是history server上看历史作业的web ui。
2. 如果你希望将metrics数据，结构化处理以后导入到，比如mysql里面，然后进行一个存储，开发一个系统对外开放
3. spark集群运行分析系统
4. 自定义metrics sink，将所有的metrics全部写入外部的你指定的存储文件中，然后定时导入到你的mysql中

**实验: 自定义metrics sink**

1. 停止集群
2. 配置`metrics.properties`文件，启用CSVSink
3. 重启集群
4. 运行一个作业，查看指定目录下的csv文件