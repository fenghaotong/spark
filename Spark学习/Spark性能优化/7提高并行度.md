# 提高并行度

- 实际上Spark集群的资源并不一定会被充分利用到，所以要尽量设置合理的并行度，来充分地利用集群的资源。才能充分提高Spark应用程序的性能。
- Spark会自动设置以文件作为输入源的RDD的并行度，依据其大小，比如HDFS，就会给每一个block创建一个partition，也依据这个设置并行度。对于reduceByKey等会发生shuffle的操作，就使用并行度最大的父RDD的并行度即可。
- 可以手动使用textFile()、parallelize()等方法的第二个参数来设置并行度；也可以使用`spark.default.parallelism`参数，来设置统一的并行度。Spark官方的推荐是，给集群中的每个cpu core设置2~3个task。
- 比如说，spark-submit设置了executor数量是10个，每个executor要求分配2个core，那么application总共会有20个core。此时可以设置`new SparkConf().set("spark.default.parallelism", "60")`来设置合理的并行度，从而充分利用资源。

![](img\提高并行度原理.png)